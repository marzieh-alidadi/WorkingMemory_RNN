# WorkingMemory_RNN

**Project Title: “The Effect of Local Learning Rules on Memory”**


Abstract: 

"A basic problem for neuroscience is: How does the brain come to produce complex, organised behaviour—for example, playing a tune or reproducing an episode sequence—without an external teacher? We know that the brain sets up an immense web of interconnected neurons, and by changing the strength of connections (synapses), learning occurs. Modelling these neuronal networks & simulating learning loops using a recurrent neural network (RNN), the entire network can self-organise & produce sequences of activity by itself. Now the question is: How do different local learning rules, like Hebbian, anti-Hebbian, & Spike-Timing-Dependent Plasticity (STDP), shape a network's synaptic organisation & attractor landscape?

We’ll train an initially random RNN on a sequence-generating task for each of these rules. To compare results, we'll use a multi-level analysis: (a) the emerging structure is examined by observing the learned synaptic weight matrix; and (b) resultant dynamics are displayed by phase plane and nullcline analysis. This enables us to explore if the rules converge to one, universal solution or produce functionally equivalent but structurally variant circuits.

We also predict that time-dependent rules like STDP will be particularly skilled at that task, successfully constructing the asymmetric synaptic arrangements necessary to guide the network activities through an ordered sequence. In contrast, relatively simple, correlational rules would have limited power to encode temporal order, resulting in less stable sequence generation and an ill-defined synaptic organisation that has poor flow directions. 

Why do all this? We present a comparative analysis of the biological plausibility and the trade-offs of root learning theories for a dynamic system. The findings will provide insights into how neural circuits self-organise and can inform the development of more autonomous, brain-inspired artificial intelligence (AI), ultimately contributing to artificial general intelligence (AGI)."


--------------------------------------------------------------------------------------------------------------------------------------------------------------


Our team in Computatinal Neuroscience course of Neuromatch Academy — The Unsupervised Minds — worked on a project titled:

**“The Effect of Local Learning Rules on Memory”**


We modeled how Hebbian and anti-Hebbian plasticity influence memory formation in recurrent neural networks, and how small differences in local rules can lead to entirely different network behaviors and memory patterns.

*- I have included the code notebook and presentation slides in this repository.*
